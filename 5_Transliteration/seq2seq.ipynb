{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021e158e-5679-439a-8510-6c9efe4ccb61",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9aa7745-b82d-4afe-8bfd-c4868c9271c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb transformers[torch] torch pandas datasets evaluate sacrebleu\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a6b9f-1968-4ba1-9223-08d89f19cd1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f1f8f3-9633-4ab4-8bbe-9a59ab81bfc5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf8df859-e870-411f-9113-748bb636cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from itertools import chain\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    dataset = datasets.load_dataset(\"colesimmons/SumTablets\")\n",
    "\n",
    "    # Oversample non-administrative examples in the training set\n",
    "    def oversample_non_administrative(dataset_, oversampling_factor):\n",
    "        admin_examples = dataset_.filter(\n",
    "            lambda example: example[\"genre\"] == \"Administrative\"\n",
    "        )\n",
    "        print(\"Num admin: \", len(admin_examples))\n",
    "\n",
    "        non_admin_examples = dataset_.filter(\n",
    "            lambda example: example[\"genre\"] != \"Administrative\"\n",
    "        )\n",
    "        print(\"Num non-admin: \", len(non_admin_examples))\n",
    "\n",
    "        oversampled_non_admin = datasets.concatenate_datasets(\n",
    "            [non_admin_examples] * oversampling_factor\n",
    "        )\n",
    "        print(\"Num non-admin after: \", len(oversampled_non_admin))\n",
    "\n",
    "        balanced = datasets.concatenate_datasets(\n",
    "            [admin_examples, oversampled_non_admin]\n",
    "        )\n",
    "        balanced = balanced.shuffle(seed=42)\n",
    "        return balanced\n",
    "\n",
    "    dataset[\"train\"] = oversample_non_administrative(dataset[\"train\"], 10)\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42)\n",
    "    dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize(\n",
    "    dataset,\n",
    "    *,\n",
    "    encoder_tokenizer,\n",
    "    decoder_tokenizer,\n",
    "    max_length=256,\n",
    "    cache_location=None,\n",
    "):\n",
    "    def _tokenize(examples):\n",
    "        inputs = encoder_tokenizer(\n",
    "            examples[\"glyphs\"],\n",
    "            # padding=\"max_length\",\n",
    "            # max_length=max_length,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "        )\n",
    "        labels = decoder_tokenizer(\n",
    "            examples[\"transliteration\"],\n",
    "            # padding=\"max_length\",\n",
    "            # max_length=max_length,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "        )\n",
    "\n",
    "        return {\"input_ids\": inputs[\"input_ids\"], \"labels\": labels[\"input_ids\"]}\n",
    "\n",
    "    # Tokenize the tablets, removing old columns\n",
    "    columns_to_remove = [\n",
    "        \"id\",\n",
    "        \"glyph_names\",\n",
    "        \"genre\",\n",
    "        \"period\",\n",
    "    ]\n",
    "    dataset_ = dataset.map(_tokenize, batched=True, remove_columns=columns_to_remove)\n",
    "\n",
    "    # These are below the max length, so they're good\n",
    "    below_max_length = dataset_[\"train\"].filter(\n",
    "        lambda example: len(example[\"input_ids\"]) <= max_length\n",
    "        and len(example[\"labels\"]) <= max_length\n",
    "    )\n",
    "    print(\"Below max length: \", len(below_max_length))\n",
    "\n",
    "    # These are above the max length, so we need to do some extra work\n",
    "    above_max_length = dataset_.filter(\n",
    "        lambda example: len(example[\"input_ids\"]) > max_length\n",
    "        or len(example[\"labels\"]) > max_length\n",
    "    )\n",
    "    print(\"Above max length: \", len(above_max_length))\n",
    "\n",
    "    def _split_overly_long_examples_by_surface(examples):\n",
    "        \"\"\"Split examples that are too long into multiple examples.\n",
    "        Try to do it by surface.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split by surface\n",
    "        glyphs = [glyphs.split(\"<SURFACE>\") for glyphs in examples[\"glyphs\"]]\n",
    "        glyphs = list(chain(*glyphs))\n",
    "        glyphs = [\"<SURFACE>\" + glyph for glyph in glyphs if glyph]\n",
    "\n",
    "        transliterations = [\n",
    "            example[\"transliteration\"].split(\"<SURFACE>\") for example in examples\n",
    "        ]\n",
    "        transliterations = list(chain(*transliterations))\n",
    "        transliterations = [\n",
    "            \"<SURFACE>\" + transliteration\n",
    "            for transliteration in transliterations\n",
    "            if transliteration\n",
    "        ]\n",
    "        return {\"glyphs\": glyphs, \"transliteration\": transliterations}\n",
    "\n",
    "    above_max_length = above_max_length.map(\n",
    "        _split_overly_long_examples_by_surface, batched=True\n",
    "    )\n",
    "    above_max_length = above_max_length.map(_tokenize, batched=True)\n",
    "\n",
    "    fixed = above_max_length.filter(\n",
    "        lambda example: len(example[\"input_ids\"]) <= max_length\n",
    "        and len(example[\"labels\"]) <= max_length\n",
    "    )\n",
    "    print(\"Fixed: \", len(fixed))\n",
    "    below_max_length = datasets.concatenate_datasets([below_max_length, fixed])\n",
    "    above_max_length = above_max_length.filter(\n",
    "        lambda example: len(example[\"input_ids\"]) > max_length\n",
    "        or len(example[\"labels\"]) > max_length\n",
    "    )\n",
    "    print(\"Still above max length: \", len(above_max_length))\n",
    "\n",
    "    if cache_location:\n",
    "        below_max_length.save_to_disk(cache_location)\n",
    "\n",
    "    return below_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31902dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_DATASET_FROM_CACHE = False\n",
    "DATASET_CACHE_LOCATION = \"./dataset\"\n",
    "\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"colesimmons/SumerianGlyphTokenizer_Roberta\"\n",
    ")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"colesimmons/SumerianTransliterationTokenizer_Roberta\"\n",
    ")\n",
    "\n",
    "if LOAD_DATASET_FROM_CACHE:\n",
    "    dataset = datasets.load_from_disk(DATASET_CACHE_LOCATION)\n",
    "else:\n",
    "    dataset = load_dataset()\n",
    "    dataset = tokenize(\n",
    "        dataset,\n",
    "        encoder_tokenizer=encoder_tokenizer,\n",
    "        decoder_tokenizer=decoder_tokenizer,\n",
    "        max_length=MAX_LENGTH,\n",
    "        cache_location=DATASET_CACHE_LOCATION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1eaa46f7-3d36-4ff2-9f3d-51c9cfcddcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 250002, 7450, 250002, 250004, 250002, 28, 4571, 9, 561, 9, 85, 9, 143, 18504, 13, 250004, 250002, 10, 2402, 71, 966, 256, 9, 192, 9, 112, 18504, 112, 304, 250004, 250002, 10, 2402, 71, 966, 256, 9, 192, 9, 112, 18504, 112, 304, 250004, 250002, 51, 9387, 9, 24854, 301, 8152, 9, 208, 17, 735, 9, 93, 9, 4807, 363, 256, 9, 402, 9, 76582, 18504, 429, 250002, 250004, 250002, 51, 9387, 9, 24854, 301, 8152, 9, 208, 17, 735, 9, 93, 9, 4807, 363, 256, 9, 402, 9, 76582, 18504, 429, 250002, 250004, 250002, 842, 794, 80, 9, 6820, 27495, 76, 4571, 9, 561, 9, 85, 9, 143, 18504, 13, 250004, 250002, 17, 735, 9, 93, 9, 4807, 363, 842, 9, 85, 9, 18, 16480, 250004, 250002, 24, 9, 232287, 34, 963, 250004, 250002, 6494, 9, 12283, 9, 13, 250004, 250002, 6, 232287, 34, 963, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 250002, 7450, 250002, 250004, 250002, 241, 22438, 11183, 6405, 339, 7842, 241, 250004, 250002, 62, 6, 58745, 23330, 6200, 363, 17676, 132, 284, 5, 17507, 16, 58745, 11666, 9267, 1514, 304, 1514, 304, 250004, 250002, 62, 6, 58745, 23330, 6200, 363, 17676, 132, 284, 5, 17507, 16, 58745, 11666, 9267, 1514, 304, 1514, 304, 250004, 250002, 11321, 981, 177, 16986, 23749, 87, 18027, 20001, 241, 6200, 304, 11666, 8678, 6, 58745, 18431, 1230, 18431, 58745, 42887, 250002, 250004, 250002, 11321, 981, 177, 16986, 23749, 87, 18027, 20001, 241, 6200, 304, 11666, 8678, 6, 58745, 18431, 1230, 18431, 58745, 42887, 250002, 250004, 250002, 11183, 6200, 7450, 72141, 3076, 22438, 11183, 6405, 339, 7842, 241, 250004, 250002, 87, 18027, 20001, 241, 6200, 304, 11183, 6405, 8992, 250004, 250002, 3076, 11183, 250004, 250002, 8678, 397, 8678, 397, 241, 250004, 250002, 11183, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "<s><MISSING> ZA<MISSING><NEWLINE><MISSING> e hu-mu-da-la2-e<NEWLINE><MISSING> a šed7 ha-ma-de2-de2<NEWLINE><MISSING> a šed7 ha-ma-de2-de2<NEWLINE><MISSING> unug-{ki}-ga igi-ni-še3 ha-ba-sug2-ge<MISSING><NEWLINE><MISSING> unug-{ki}-ga igi-ni-še3 ha-ba-sug2-ge<MISSING><NEWLINE><MISSING> muš za-gin3-na hu-mu-da-la2-e<NEWLINE><MISSING> igi-ni-še3 mu-da-tuš<NEWLINE><MISSING> na-ŋu10<NEWLINE><MISSING> bad-bad-e<NEWLINE><MISSING> ŋu10</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<s><MISSING> ZA<MISSING><NEWLINE><MISSING> E HU MU DA LAL E<NEWLINE><MISSING> A |MUŠ3×(A.DI)| HA MA DE2 DE2<NEWLINE><MISSING> A |MUŠ3×(A.DI)| HA MA DE2 DE2<NEWLINE><MISSING> AB@g KI GA IGI NI EŠ2 HA BA |DU&DU| GI<MISSING><NEWLINE><MISSING> AB@g KI GA IGI NI EŠ2 HA BA |DU&DU| GI<MISSING><NEWLINE><MISSING> MUŠ ZA KUR NA HU MU DA LAL E<NEWLINE><MISSING> IGI NI EŠ2 MU DA KU<NEWLINE><MISSING> NA MU<NEWLINE><MISSING> BAD BAD E<NEWLINE><MISSING> MU</s>\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "print(example)\n",
    "print(encoder_tokenizer.decode(example[\"input_ids\"]))\n",
    "print(decoder_tokenizer.decode([t for t in example[\"labels\"] if t != -100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c91e0d-7a3b-4660-853d-774db7fbc8e9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dc08d19-546c-4a73-917f-385ce2387de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"d50c52de8f8f7a0f7afebb827fbfbdf1e506cb2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d26a7651-d609-4c88-be82-cedb3f6dd971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:726] 2024-03-03 23:27:00,780 >> loading configuration file /workspace/results/encoder/checkpoint-1200/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-03 23:27:00,783 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/workspace/results/encoder/checkpoint-1200\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250005\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3254] 2024-03-03 23:27:00,787 >> loading weights file /workspace/results/encoder/checkpoint-1200/model.safetensors\n",
      "[INFO|modeling_utils.py:3982] 2024-03-03 23:27:23,571 >> Some weights of the model checkpoint at /workspace/results/encoder/checkpoint-1200 were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3994] 2024-03-03 23:27:23,671 >> Some weights of XLMRobertaModel were not initialized from the model checkpoint at /workspace/results/encoder/checkpoint-1200 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|configuration_utils.py:726] 2024-03-03 23:27:23,769 >> loading configuration file /workspace/results/encoder/checkpoint-1200/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-03 23:27:23,772 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/workspace/results/encoder/checkpoint-1200\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250005\n",
      "}\n",
      "\n",
      "[INFO|modeling_encoder_decoder.py:506] 2024-03-03 23:27:23,773 >> Initializing /workspace/results/encoder/checkpoint-1200 as a decoder model. Cross attention layers are added to /workspace/results/encoder/checkpoint-1200 and randomly initialized if /workspace/results/encoder/checkpoint-1200's architecture allows for cross attention layers.\n",
      "[INFO|modeling_utils.py:3254] 2024-03-03 23:27:23,776 >> loading weights file /workspace/results/encoder/checkpoint-1200/model.safetensors\n",
      "[INFO|configuration_utils.py:845] 2024-03-03 23:27:23,795 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3992] 2024-03-03 23:27:41,070 >> All model checkpoint weights were used when initializing XLMRobertaForCausalLM.\n",
      "\n",
      "[WARNING|modeling_utils.py:3994] 2024-03-03 23:27:41,169 >> Some weights of XLMRobertaForCausalLM were not initialized from the model checkpoint at /workspace/results/encoder/checkpoint-1200 and are newly initialized: ['roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|modeling_utils.py:3544] 2024-03-03 23:27:41,370 >> Generation config file not found, using a generation config created from the model config.\n",
      "[INFO|configuration_encoder_decoder.py:102] 2024-03-03 23:27:41,691 >> Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
      "[INFO|configuration_utils.py:845] 2024-03-03 23:27:41,696 >> Generate config GenerationConfig {}\n",
      "\n",
      "[INFO|modeling_utils.py:1875] 2024-03-03 23:27:42,115 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 250005. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "[INFO|modeling_utils.py:1875] 2024-03-03 23:27:42,196 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 250005. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "MODELS_DIR = \"./models\"\n",
    "\n",
    "\n",
    "def load_new_model(encoder_path: str, decoder_path: str):\n",
    "    model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        encoder_path, decoder_path\n",
    "    )\n",
    "    model.encoder.resize_token_embeddings(len(encoder_tokenizer))\n",
    "    model.decoder.resize_token_embeddings(len(decoder_tokenizer))\n",
    "    model.decoder.config.is_decoder = True\n",
    "    model.decoder.config.add_cross_attention = True\n",
    "    model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "    model.config.pad_token_id = decoder_tokenizer.pad_token_id\n",
    "\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    *,\n",
    "    encoder_path: str,\n",
    "    decoder_path: str,\n",
    "    model_name: str,\n",
    "):\n",
    "    if encoder_path and decoder_path:\n",
    "        return load_new_model(encoder_path, decoder_path)\n",
    "    if model_name:\n",
    "        return EncoderDecoderModel.from_pretrained(model_name)\n",
    "    raise ValueError(\"Must provide either encoder_path and decoder_path or model_name\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    *,\n",
    "    model: EncoderDecoderModel,\n",
    "    run_name: str,\n",
    "    lr: float,\n",
    "    num_epochs: int,\n",
    "    train_batch_size: int,\n",
    "    eval_batch_size: int,\n",
    "    warmup_steps: int,\n",
    "    eval_steps: int,\n",
    "):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        # Run info\n",
    "        output_dir=f\"{MODELS_DIR}/{run_name}\",\n",
    "        run_name=run_name,\n",
    "        # Logging\n",
    "        logging_steps=10,\n",
    "        # Saving\n",
    "        save_steps=200,\n",
    "        save_total_limit=5,\n",
    "        # Train\n",
    "        bf16=True,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        # Eval\n",
    "        eval_steps=eval_steps,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=decoder_tokenizer.model_max_length,\n",
    "        # Return model\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        encoder_tokenizer, model=model, pad_to_multiple_of=128, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        tokenizer=encoder_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"transliteration\",\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    trainer.save_model(f\"{MODELS_DIR}/{run_name}/best_model\")\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba701d1c-14c3-4b47-bd45-5508e57ca947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1902] 2024-03-03 23:39:02,642 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1611] 2024-03-03 23:39:02,644 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/03/2024 23:39:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/03/2024 23:39:02 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=100,\n",
      "eval_steps=100,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=512,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/workspace/results/enc-dec-1/runs/Mar03_23-39-02_d02dda2f8cc3,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=30,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/workspace/results/enc-dec-1,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=enc-dec-1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "03/03/2024 23:39:02 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1812] 2024-03-03 23:39:03,326 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2024-03-03 23:39:03,327 >>   Num examples = 2,502\n",
      "[INFO|trainer.py:1814] 2024-03-03 23:39:03,329 >>   Num Epochs = 30\n",
      "[INFO|trainer.py:1815] 2024-03-03 23:39:03,329 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1818] 2024-03-03 23:39:03,331 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1819] 2024-03-03 23:39:03,332 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1820] 2024-03-03 23:39:03,333 >>   Total optimization steps = 9,390\n",
      "[INFO|trainer.py:1821] 2024-03-03 23:39:03,336 >>   Number of trainable parameters = 584,710,293\n",
      "[INFO|integration_utils.py:722] 2024-03-03 23:39:03,339 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3511' max='9390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3511/9390 58:54 < 1:38:41, 0.99 it/s, Epoch 11.21/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.616500</td>\n",
       "      <td>4.555409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.565700</td>\n",
       "      <td>4.572995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.501400</td>\n",
       "      <td>4.622006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.561000</td>\n",
       "      <td>4.635164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.529900</td>\n",
       "      <td>4.573621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.512200</td>\n",
       "      <td>5.031684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.534200</td>\n",
       "      <td>4.838830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.464000</td>\n",
       "      <td>7.888671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.450200</td>\n",
       "      <td>7.712603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.453300</td>\n",
       "      <td>6.497380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.445100</td>\n",
       "      <td>8.339343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.475100</td>\n",
       "      <td>7.721612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.478700</td>\n",
       "      <td>8.893098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.389700</td>\n",
       "      <td>8.834698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.447500</td>\n",
       "      <td>10.437386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.362500</td>\n",
       "      <td>10.434099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.401400</td>\n",
       "      <td>8.942245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.384100</td>\n",
       "      <td>8.604218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.343500</td>\n",
       "      <td>11.918034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.332500</td>\n",
       "      <td>10.937617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.361000</td>\n",
       "      <td>10.620468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.358500</td>\n",
       "      <td>10.480321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.358000</td>\n",
       "      <td>13.232202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.340900</td>\n",
       "      <td>10.489638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.319800</td>\n",
       "      <td>13.705954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.376000</td>\n",
       "      <td>14.659733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.306900</td>\n",
       "      <td>10.969790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>4.308400</td>\n",
       "      <td>11.285498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.291900</td>\n",
       "      <td>14.643496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.246000</td>\n",
       "      <td>13.718601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>4.270200</td>\n",
       "      <td>15.267035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.266900</td>\n",
       "      <td>12.448635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>4.175500</td>\n",
       "      <td>15.303770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>4.219800</td>\n",
       "      <td>13.768637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.207300</td>\n",
       "      <td>11.257402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3376] 2024-03-03 23:40:21,015 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:40:21,016 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:40:21,017 >>   Batch size = 8\n",
      "[WARNING|trainer.py:2492] 2024-03-03 23:40:29,452 >> Checkpoint destination directory /workspace/results/enc-dec-1/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:40:29,454 >> Saving model checkpoint to /workspace/results/enc-dec-1/checkpoint-100\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:40:29,468 >> Configuration saved in /workspace/results/enc-dec-1/checkpoint-100/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:40:29,480 >> Configuration saved in /workspace/results/enc-dec-1/checkpoint-100/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:40:34,495 >> Model weights saved in /workspace/results/enc-dec-1/checkpoint-100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:40:34,510 >> tokenizer config file saved in /workspace/results/enc-dec-1/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:40:34,521 >> Special tokens file saved in /workspace/results/enc-dec-1/checkpoint-100/special_tokens_map.json\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:42:02,104 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:42:02,105 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:42:02,106 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:42:10,538 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-200\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:42:10,552 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-200/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:42:10,562 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-200/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:42:15,166 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-200/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:42:15,177 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:42:15,187 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-200/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:42:24,376 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:43:42,266 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:43:42,268 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:43:42,269 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:43:50,701 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-300\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:43:50,713 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-300/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:43:50,722 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-300/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:43:55,516 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:43:55,528 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:43:55,537 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-300/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:44:05,054 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:45:22,748 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:45:22,750 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:45:22,751 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:45:31,176 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-400\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:45:31,191 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-400/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:45:31,204 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-400/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:45:35,829 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-400/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:45:35,842 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:45:35,852 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-400/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:45:45,416 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:47:03,339 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:47:03,340 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:47:03,341 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:47:11,768 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-500\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:47:11,780 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:47:11,790 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:47:16,385 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:47:16,398 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:47:16,408 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:47:25,989 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:48:43,984 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:48:43,986 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:48:43,987 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:48:52,422 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-600\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:48:52,436 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-600/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:48:52,445 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-600/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:48:57,338 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-600/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:48:57,351 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:48:57,361 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-600/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:49:07,310 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:50:25,012 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:50:25,013 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:50:25,015 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:50:33,443 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-700\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:50:33,457 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-700/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:50:33,468 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-700/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:50:37,974 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-700/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:50:37,986 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:50:37,995 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-700/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:50:46,855 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-700] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:52:04,746 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:52:04,748 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:52:04,749 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:52:13,174 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-800\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:52:13,185 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-800/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:52:13,194 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-800/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:52:17,736 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-800/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:52:17,749 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:52:17,760 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-800/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:52:26,991 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-800] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:53:45,338 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:53:45,339 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:53:45,339 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:53:53,765 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-900\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:53:53,776 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-900/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:53:53,787 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-900/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:53:58,457 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-900/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:53:58,470 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:53:58,480 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-900/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:54:08,526 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-900] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:55:26,216 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:55:26,217 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:55:26,218 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:55:34,651 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1000\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:55:34,664 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:55:34,674 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:55:39,405 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:55:39,417 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:55:39,427 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:55:49,339 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:57:07,227 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:57:07,228 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:57:07,229 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:57:15,662 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1100\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:57:15,677 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1100/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:57:15,689 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1100/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:57:20,282 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:57:20,302 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:57:20,312 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1100/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:57:30,705 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-03 23:58:48,539 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-03 23:58:48,541 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-03 23:58:48,542 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-03 23:58:56,978 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1200\n",
      "[INFO|configuration_utils.py:473] 2024-03-03 23:58:56,995 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1200/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-03 23:58:57,008 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1200/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-03 23:59:01,747 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1200/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-03 23:59:01,760 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-03 23:59:01,771 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1200/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-03 23:59:11,871 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:00:29,842 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:00:29,844 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:00:29,845 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:00:38,274 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1300\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:00:38,287 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1300/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:00:38,296 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1300/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:00:42,997 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:00:43,014 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:00:43,028 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1300/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:00:52,379 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:02:10,284 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:02:10,285 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:02:10,286 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:02:18,718 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1400\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:02:18,734 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1400/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:02:18,745 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1400/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:02:23,364 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1400/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:02:23,379 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:02:23,389 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1400/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:02:32,978 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:03:50,884 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:03:50,885 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:03:50,886 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:03:59,310 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1500\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:03:59,323 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:03:59,335 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1500/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:04:03,748 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:04:03,761 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:04:03,769 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:04:13,067 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:05:30,726 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:05:30,728 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:05:30,729 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:05:39,157 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1600\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:05:39,172 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1600/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:05:39,182 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1600/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:05:43,680 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1600/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:05:43,694 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:05:43,707 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1600/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:05:53,003 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:07:10,892 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:07:10,893 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:07:10,894 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:07:19,324 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1700\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:07:19,340 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1700/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:07:19,350 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1700/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:07:24,081 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1700/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:07:24,095 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:07:24,106 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1700/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:07:41,458 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1700] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:08:59,631 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:08:59,632 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:08:59,633 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:09:08,058 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1800\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:09:08,071 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1800/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:09:08,081 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1800/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:09:12,785 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1800/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:09:12,801 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:09:12,811 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1800/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:09:22,684 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1800] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:10:40,394 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:10:40,396 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:10:40,397 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:10:48,839 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-1900\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:10:48,849 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1900/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:10:48,859 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-1900/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:10:53,522 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-1900/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:10:53,535 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:10:53,547 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-1900/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:11:03,090 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-1900] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:12:20,826 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:12:20,827 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:12:20,828 >>   Batch size = 8\n",
      "[WARNING|trainer.py:2492] 2024-03-04 00:12:29,253 >> Checkpoint destination directory /workspace/results/enc-dec-1/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:12:29,254 >> Saving model checkpoint to /workspace/results/enc-dec-1/checkpoint-2000\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:12:29,268 >> Configuration saved in /workspace/results/enc-dec-1/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:12:29,278 >> Configuration saved in /workspace/results/enc-dec-1/checkpoint-2000/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:12:34,078 >> Model weights saved in /workspace/results/enc-dec-1/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:12:34,092 >> tokenizer config file saved in /workspace/results/enc-dec-1/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:12:34,103 >> Special tokens file saved in /workspace/results/enc-dec-1/checkpoint-2000/special_tokens_map.json\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:14:01,380 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:14:01,382 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:14:01,383 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:14:09,808 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2100\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:14:09,822 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2100/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:14:09,836 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2100/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:14:14,192 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:14:14,210 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:14:14,219 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2100/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:14:23,860 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:15:41,714 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:15:41,716 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:15:41,717 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:15:50,150 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2200\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:15:50,164 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2200/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:15:50,174 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2200/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:15:55,093 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2200/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:15:55,107 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:15:55,120 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2200/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:16:04,872 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:17:22,763 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:17:22,765 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:17:22,766 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:17:31,196 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2300\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:17:31,211 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2300/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:17:31,222 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2300/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:17:35,476 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:17:35,489 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:17:35,503 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2300/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:17:44,493 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:19:02,367 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:19:02,369 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:19:02,370 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:19:10,796 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2400\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:19:10,813 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2400/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:19:10,824 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2400/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:19:15,498 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2400/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:19:15,513 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:19:15,522 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2400/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:19:25,203 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:20:43,059 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:20:43,060 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:20:43,062 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:20:51,494 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2500\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:20:51,510 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:20:51,522 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2500/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:20:55,847 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:20:55,860 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:20:55,870 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:21:04,973 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:22:22,654 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:22:22,656 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:22:22,657 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:22:31,087 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2600\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:22:31,103 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2600/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:22:31,113 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2600/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:22:35,677 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2600/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:22:35,691 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:22:35,703 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2600/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:22:44,997 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:24:03,146 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:24:03,147 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:24:03,148 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:24:11,576 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2700\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:24:11,590 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2700/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:24:11,604 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2700/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:24:15,983 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2700/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:24:15,995 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:24:16,006 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2700/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:24:25,123 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:25:43,031 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:25:43,032 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:25:43,034 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:25:51,468 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2800\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:25:51,481 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2800/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:25:51,492 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2800/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:25:56,074 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2800/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:25:56,087 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:25:56,106 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2800/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:26:05,790 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2700] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:27:23,481 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:27:23,483 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:27:23,484 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:27:31,914 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-2900\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:27:31,926 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2900/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:27:31,937 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-2900/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:27:36,671 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-2900/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:27:36,686 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:27:36,695 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-2900/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:27:46,638 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2800] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:29:04,510 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:29:04,512 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:29:04,513 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:29:12,941 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-3000\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:29:12,955 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:29:12,964 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3000/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:29:17,959 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:29:17,972 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:29:17,993 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:29:27,683 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-2900] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:30:45,543 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:30:45,544 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:30:45,546 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:30:53,971 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-3100\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:30:53,985 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3100/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:30:53,995 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3100/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:30:58,285 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-3100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:30:58,298 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:30:58,312 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3100/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:31:07,551 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-3000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:32:25,231 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:32:25,233 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:32:25,234 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:32:33,665 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-3200\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:32:33,678 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3200/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:32:33,688 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3200/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:32:38,594 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-3200/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:32:38,608 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:32:38,616 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3200/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:32:48,621 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-3100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:34:06,515 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:34:06,517 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:34:06,518 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:34:14,949 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-3300\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:34:14,963 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3300/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:34:14,974 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3300/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:34:19,601 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-3300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:34:19,615 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:34:19,624 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3300/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:34:29,094 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-3200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:35:47,006 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:35:47,007 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:35:47,008 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:35:55,437 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-3400\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:35:55,450 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3400/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:35:55,462 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3400/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:36:00,125 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-3400/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:36:00,138 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:36:00,147 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3400/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:36:09,787 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-3300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "[INFO|trainer.py:3376] 2024-03-04 00:37:27,793 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-04 00:37:27,795 >>   Num examples = 278\n",
      "[INFO|trainer.py:3381] 2024-03-04 00:37:27,796 >>   Batch size = 8\n",
      "[INFO|trainer.py:3067] 2024-03-04 00:37:36,229 >> Saving model checkpoint to /workspace/results/enc-dec-1/tmp-checkpoint-3500\n",
      "[INFO|configuration_utils.py:473] 2024-03-04 00:37:36,243 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:614] 2024-03-04 00:37:36,257 >> Configuration saved in /workspace/results/enc-dec-1/tmp-checkpoint-3500/generation_config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-03-04 00:37:40,774 >> Model weights saved in /workspace/results/enc-dec-1/tmp-checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-04 00:37:40,789 >> tokenizer config file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-04 00:37:40,800 >> Special tokens file saved in /workspace/results/enc-dec-1/tmp-checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:3159] 2024-03-04 00:37:50,384 >> Deleting older checkpoint [/workspace/results/enc-dec-1/checkpoint-3400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ebe124-6d9e-4d0d-91fb-7bd42b26c8a3",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "11a4e87c-1104-4129-900e-b5bfeaafefea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import os\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "preds_dir = f\"{BASE_PATH}/predictions\"\n",
    "os.makedirs(preds_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, decoder_tokenizer.pad_token_id)\n",
    "    decoded_preds = decoder_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, decoder_tokenizer.pad_token_id)\n",
    "    decoded_labels = decoder_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(preds_dir, \"predictions.txt\"), \"w\", encoding=\"utf-8\"\n",
    "    ) as outfile:\n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            outfile.write(pred + \"\\n\")\n",
    "            outfile.write(label[0] + \"\\n\")  # Assuming labels are lists of lists\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != decoder_tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "def _compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    return {\"bleu\": 0}\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
