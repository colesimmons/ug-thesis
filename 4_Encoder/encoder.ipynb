{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8baa3870-5591-4677-9177-93600977ef6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59351085-dfb6-4fdb-b546-29a18fb8c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.1.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (69.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb transformers[torch] torch pandas datasets evaluate sacrebleu scikit-learn\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2074ef48-0499-4f2f-9ac7-ba5712013b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import wandb\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaForMaskedLM,\n",
    "    XLMRobertaXLForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizerFast\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99d582-9708-42eb-9264-d0526fc8f9c0",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa6a1ca-df92-4a4e-ad9d-8575510db119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimmons\u001b[0m (\u001b[33mcoles\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"d50c52de8f8f7a0f7afebb827fbfbdf1e506cb2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0f186-bd3f-4f4b-8209-9f060646fcdf",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493b1a20-a378-4f9b-bb00-78f4fe423e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_PATH = \"/workspace/\"\n",
    "#MODEL_ID = \"./results/encoder/best_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d1f86-0255-463f-bcd3-f3c63a6460be",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d61778d-cf6c-473e-9f10-dfbe982a65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'period', 'genre', 'transliteration', 'glyph_names', 'glyphs'],\n",
       "        num_rows: 82452\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'period', 'genre', 'transliteration', 'glyph_names', 'glyphs'],\n",
       "        num_rows: 4577\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'period', 'genre', 'transliteration', 'glyph_names', 'glyphs'],\n",
       "        num_rows: 4577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = datasets.load_dataset(\"colesimmons/SumTablets\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca8ff27-d23d-45e7-aaf5-0c923e282740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'P119622',\n",
       " 'period': 'Ur III',\n",
       " 'genre': 'Administrative',\n",
       " 'transliteration': '<SURFACE>\\n2(u) udu\\nki nin-Å¡eâ‚ƒ\\nki uÅ¡-mu-ta\\nkiÅ¡ibâ‚ƒ ensiâ‚‚-ka\\n<SURFACE>\\nÅ¡aâ‚ƒ uriâ‚…{ki}ma\\niti ezem-mah\\nmu en eridu{ki} ba-hun\\n<SURFACE>\\n<COLUMN>\\n{d}amar{d}suen\\nlugal kal-ga\\nlugal uriâ‚…{ki}ma\\nlugal an ub-da limmuâ‚‚-ba\\n<COLUMN>\\na-kal-la\\nensiâ‚‚\\numma{ki}\\n<unk> zu',\n",
       " 'glyph_names': '<SURFACE> \\n |U.U| LU \\n KI |SAL.TUGâ‚‚| EÅ â‚‚ \\n KI UÅ  MU TA \\n DUB |PA.TE.SI| KA \\n <SURFACE> \\n Å Aâ‚ƒ |Å EÅ .AB| KI MA \\n |UDÃ—(U.U.U)| EZEN MAH \\n MU EN NUN KI BA EÅ â‚‚ \\n <SURFACE> \\n <COLUMN> \\n AN AMAR AN |EN.ZU| \\n LUGAL KAL GA \\n LUGAL |Å EÅ .AB| KI MA \\n LUGAL AN UB DA LIMMUâ‚‚ BA \\n <COLUMN> \\n A KAL LA \\n |PA.TE.SI| \\n |GIÅ .KUÅ Uâ‚‚| KI \\n <unk> ZU',\n",
       " 'glyphs': '<SURFACE>\\nğ’™ğ’‡»\\nğ’† ğ’ğ’‚ \\nğ’† ğ’‘ğ’ˆ¬ğ’‹«\\nğ’¾ğ’‰ºğ’‹¼ğ’‹›ğ’…—\\n<SURFACE>\\nğ’Š®ğ’‹€ğ’€Šğ’† ğ’ˆ \\nğ’Œ—ğ’‚¡ğ’ˆ¤\\nğ’ˆ¬ğ’‚—ğ’‰£ğ’† ğ’€ğ’‚ \\n<SURFACE>\\n<COLUMN>\\nğ’€­ğ’€«ğ’€­ğ’‚—ğ’ª\\nğ’ˆ—ğ’†—ğ’‚µ\\nğ’ˆ—ğ’‹€ğ’€Šğ’† ğ’ˆ \\nğ’ˆ—ğ’€­ğ’Œ’ğ’•ğ’‡¹ğ’€\\n<COLUMN>\\nğ’€€ğ’†—ğ’†·\\nğ’‰ºğ’‹¼ğ’‹›\\nğ’„‘ğ’†µğ’† \\n<unk>ğ’ª'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36a92aa-e1f0-43cd-913f-9bcec915054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num admin:  77193\n",
      "Num non-admin:  5259\n",
      "Num non-admin after:  52590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'period', 'genre', 'transliteration', 'glyph_names', 'glyphs'],\n",
       "        num_rows: 129783\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'period', 'genre', 'transliteration', 'glyph_names', 'glyphs'],\n",
       "        num_rows: 4577\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'period', 'genre', 'transliteration', 'glyph_names', 'glyphs'],\n",
       "        num_rows: 4577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oversample_non_administrative(dataset_):\n",
    "    admin_examples = dataset_.filter(lambda example: example['genre'] == 'Administrative')\n",
    "    print(\"Num admin: \", len(admin_examples))\n",
    "    non_admin_examples = dataset_.filter(lambda example: example['genre'] != 'Administrative')\n",
    "    print(\"Num non-admin: \", len(non_admin_examples))\n",
    "    \n",
    "    oversampling_factor = 10\n",
    "    \n",
    "    oversampled_non_admin = datasets.concatenate_datasets([non_admin_examples] * oversampling_factor)\n",
    "    print(\"Num non-admin after: \", len(oversampled_non_admin))\n",
    "    \n",
    "    balanced_dataset = datasets.concatenate_datasets([admin_examples, oversampled_non_admin])\n",
    "    balanced_dataset = balanced_dataset.shuffle(seed=42)\n",
    "    return balanced_dataset\n",
    "\n",
    "oversampled = raw_dataset\n",
    "oversampled[\"train\"] = oversample_non_administrative(oversampled['train'])\n",
    "oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "076e1dc9-8665-47e3-9569-fe36ae502f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d49775a3b48484889dbc3bb89d81820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f5ed8b106449ebbc823c79bdf30e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f75a528cc24dc6bab180bf52b1d575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4282fdabfe4648a4d37cb4b22b57d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6bc86c9ab54ca4a716903181ace2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='colesimmons/SumerianGlyphTokenizer_Roberta', vocab_size=632, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<SURFACE>', '<COLUMN>', '<BLANK_SPACE>', '<RULING>', '\\n', '...']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<SURFACE>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<COLUMN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<BLANK_SPACE>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<RULING>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"...\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"colesimmons/SumerianGlyphTokenizer_Roberta\", force_download=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43995235-b64a-422e-8dd9-d455ba4cebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(dataset, tokenizer):\n",
    "    \"\"\"Tokenize the tablets, remove unused columns,\n",
    "    and split into chunks of length seq_len\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len = 64\n",
    "    \n",
    "    def tokenize(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"glyphs\"],\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "        )\n",
    "        tokenized[\"special_tokens_mask\"] = [\n",
    "            tokenizer.get_special_tokens_mask(input_ids, already_has_special_tokens=True)\n",
    "            for input_ids in tokenized[\"input_ids\"]\n",
    "        ]\n",
    "        return tokenized\n",
    "\n",
    "    def split_into_chunks(examples):  \n",
    "        chunks = {key: [] for key in examples.keys()}\n",
    "        for k in examples.keys():\n",
    "            for example in examples[k]:                \n",
    "                if len(example) <= seq_len:\n",
    "                    chunks[k].append(example)\n",
    "                elif len(example) <= 2 * seq_len:\n",
    "                    chunks[k].append(example[:seq_len])\n",
    "                    chunks[k].append(example[-seq_len:])\n",
    "                else:\n",
    "                    for i in range(0, len(example) - seq_len + 1, seq_len - 10):\n",
    "                        chunks[k].append(example[i:i + seq_len])\n",
    "                    chunks[k].append(example[-seq_len:])\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    columns_to_remove = dataset.column_names[\"train\"]\n",
    "    dataset = dataset.map(tokenize, batched=True, remove_columns=columns_to_remove)\n",
    "    dataset = dataset.map(split_into_chunks, batched=True)\n",
    "    return dataset\n",
    "\n",
    "dataset = tokenize_and_chunk(oversampled, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308e8a9c-2a4b-4059-92fa-2174869d996d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 334679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 9826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 10149\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c08bf7-2369-45d3-ab6c-96cd30d13802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 6, 5, 476, 560, 196, 482, 344, 113, 5, 178, 281, 267, 464, 12, 113, 5, 161, 203, 399, 225, 128, 5, 565, 196, 482, 560, 113, 5, 10, 474, 3, 5, 6, 5, 10, 58, 64, 5, 389, 330, 345, 334, 373, 5, 92, 275, 126, 366, 5, 345, 209, 209, 376, 401, 267, 10, 219, 496, 5, 6, 5, 275, 126], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0]}\n",
      "<s><SURFACE>\n",
      "ğ’Œ‹ğ’ˆğ’„¨ğ’Œ“ğ’ˆ«ğ’‚ \n",
      "ğ’„‘ğ’†µğ’† ğ’‹¼ğ’€€ğ’‚ \n",
      "ğ’ƒ»ğ’„¯ğ’Šğ’…ğ’‚·\n",
      "ğ’ğ’„¨ğ’Œ“ğ’ˆğ’‚ \n",
      "...ğ’Œˆ<unk>\n",
      "<SURFACE>\n",
      "...ğ’ğ’•\n",
      "ğ’‰ºğ’ˆ—ğ’ˆ¬ğ’ˆ ğ’‰˜\n",
      "ğ’¾ğ’†¬ğ’‚µğ’‰Œ\n",
      "ğ’ˆ¬ğ’„·ğ’„·ğ’‰¡ğ’Š‘ğ’† ...ğ’…†ğ’Œ¨\n",
      "<SURFACE>\n",
      "ğ’†¬ğ’‚µ\n"
     ]
    }
   ],
   "source": [
    "_idx = 0\n",
    "print(dataset[\"train\"][_idx])\n",
    "print(tokenizer.decode(dataset[\"train\"][_idx][\"input_ids\"], skip_special_tokens=False))\n",
    "#print()\n",
    "#print(tokenizer.convert_ids_to_tokens(472))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ccf17f-8002-4996-96f4-e2cfa36f7753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 334679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 9826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 10149\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeds 42, 20, 10, 5, 3, 50\n",
    "# seeds 1 (emb + lm head), 2, 3,\n",
    "# skip 8\n",
    "shuffled = dataset.shuffle(seed=10)\n",
    "shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b95021-bfd3-4195-9b89-dc87d8c8df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_to_token = {id: token for token, id in tokenizer.get_vocab().items()}\n",
    "#print(id_to_token[250004])\n",
    "#print(id_to_token[250003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebfc22a7-eb1a-4797-8495-23b15ca12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '</s>', '<s>', '<BLANK_SPACE>', '<SURFACE>', '...', '<mask>', '<COLUMN>', '<RULING>']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in tokenizer.get_vocab().keys() if len(k) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665248b5-20af-4dab-bbad-57ed3c4568bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.makedirs(\"/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6032814-df3f-46e7-8955-1d8c026c664d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ad3230-21bd-4785-abcd-e23ae218b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ METRICS -----\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction):\n",
    "    preds, labels = eval_preds\n",
    "    labels = labels.reshape(-1)\n",
    "    preds = preds.reshape(-1)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "# ------ MODEL -----\n",
    "def init_model(\n",
    "    *,\n",
    "    model_id,\n",
    "    attn_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "):\n",
    "    \n",
    "    model = XLMRobertaForMaskedLM.from_pretrained(\n",
    "        model_id,\n",
    "        attention_probs_dropout_prob=attn_dropout_prob,\n",
    "        hidden_dropout_prob=hidden_dropout_prob,\n",
    "        #num_hidden_layers=num_hidden_layers,\n",
    "        #num_attention_heads=num_attention_heads,\n",
    "    )\n",
    "    print(\"Loading: \", model_id)\n",
    "    \n",
    "    # Freeze all layers in the model\n",
    "    if model_id.startswith(\"/\"):\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=128)\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.roberta.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        #for name, param in model.named_parameters():\n",
    "            #if \"layer.\" in name and int(name.split(\".\")[3]) >= 2:\n",
    "                #param.requires_grad = True\n",
    "        for param in model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        # resize embeddings for new vocab\n",
    "        print(\"Num parameters before: \", sum(p.numel() for p in model.parameters()))\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=128)\n",
    "        print(\"Num parameters after: \", sum(p.numel() for p in model.parameters()))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze the embedding layer\n",
    "        for param in model.roberta.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_data_collator(*, mlm_prob):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm_probability=mlm_prob\n",
    "    )\n",
    "\n",
    "\n",
    "def init_training_args(\n",
    "    *,\n",
    "    eval_batch_size,\n",
    "    eval_steps,\n",
    "    lr,\n",
    "    logging_steps,\n",
    "    num_epochs,\n",
    "    run_name,\n",
    "    save_steps,\n",
    "    save_total_limit,\n",
    "    train_batch_size,\n",
    "    warmup_steps,\n",
    "):\n",
    "    return TrainingArguments(\n",
    "        # Run info\n",
    "        output_dir=f\"/models/{run_name}\",\n",
    "        run_name=run_name,\n",
    "        # Logging\n",
    "        logging_steps=logging_steps,\n",
    "        # Saving\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        # Train\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        # Eval\n",
    "        eval_steps=eval_steps,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        # Return model\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f674f884-e89b-4b5c-85e4-1dc1355f1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    *,\n",
    "    run_name,\n",
    "    model_id,\n",
    "    attn_dropout_prob,\n",
    "    hidden_dropout_prob,\n",
    "    lr,\n",
    "    mlm_prob,\n",
    "    # Meta\n",
    "    num_epochs,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=64,\n",
    "    warmup_steps,\n",
    "):\n",
    "    model = init_model(\n",
    "        model_id=model_id,\n",
    "        attn_dropout_prob=attn_dropout_prob,\n",
    "        hidden_dropout_prob=hidden_dropout_prob,\n",
    "    )\n",
    "    \n",
    "    data_collator = init_data_collator(\n",
    "        mlm_prob=mlm_prob,\n",
    "    )\n",
    "\n",
    "    training_args = init_training_args(\n",
    "        eval_batch_size=eval_batch_size,\n",
    "        eval_steps=100,\n",
    "        lr=lr,\n",
    "        logging_steps=10,\n",
    "        num_epochs=num_epochs,\n",
    "        run_name=run_name,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        train_batch_size=train_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=shuffled[\"train\"],\n",
    "        eval_dataset=shuffled[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"glyph_encoder\",\n",
    "        #notes=\"test\",\n",
    "        config={\"mlm_prob\": mlm_prob}\n",
    "    )\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(f\"/models/{run_name}/best_model\")\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    #try:\n",
    "    #    perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "    #except OverflowError:\n",
    "    #    perplexity = float(\"inf\")\n",
    "\n",
    "    #metrics[\"perplexity\"] = perplexity\n",
    "    #wandb.log(metrics)\n",
    "\n",
    "    #trainer.log_metrics(\"eval\", metrics)\n",
    "    #trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4df756-6f8d-4307-8d9f-f82ebae01856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /models/base_2_all_layers/best_model\n",
      "roberta.embeddings.word_embeddings.weight: True\n",
      "roberta.embeddings.position_embeddings.weight: True\n",
      "roberta.embeddings.token_type_embeddings.weight: True\n",
      "roberta.embeddings.LayerNorm.weight: True\n",
      "roberta.embeddings.LayerNorm.bias: True\n",
      "roberta.encoder.layer.0.attention.self.query.weight: True\n",
      "roberta.encoder.layer.0.attention.self.query.bias: True\n",
      "roberta.encoder.layer.0.attention.self.key.weight: True\n",
      "roberta.encoder.layer.0.attention.self.key.bias: True\n",
      "roberta.encoder.layer.0.attention.self.value.weight: True\n",
      "roberta.encoder.layer.0.attention.self.value.bias: True\n",
      "roberta.encoder.layer.0.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.0.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.0.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.0.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.0.output.dense.weight: True\n",
      "roberta.encoder.layer.0.output.dense.bias: True\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.1.attention.self.query.weight: True\n",
      "roberta.encoder.layer.1.attention.self.query.bias: True\n",
      "roberta.encoder.layer.1.attention.self.key.weight: True\n",
      "roberta.encoder.layer.1.attention.self.key.bias: True\n",
      "roberta.encoder.layer.1.attention.self.value.weight: True\n",
      "roberta.encoder.layer.1.attention.self.value.bias: True\n",
      "roberta.encoder.layer.1.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.1.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.1.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.1.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.1.output.dense.weight: True\n",
      "roberta.encoder.layer.1.output.dense.bias: True\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.2.attention.self.query.weight: True\n",
      "roberta.encoder.layer.2.attention.self.query.bias: True\n",
      "roberta.encoder.layer.2.attention.self.key.weight: True\n",
      "roberta.encoder.layer.2.attention.self.key.bias: True\n",
      "roberta.encoder.layer.2.attention.self.value.weight: True\n",
      "roberta.encoder.layer.2.attention.self.value.bias: True\n",
      "roberta.encoder.layer.2.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.2.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.2.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.2.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.2.output.dense.weight: True\n",
      "roberta.encoder.layer.2.output.dense.bias: True\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.3.attention.self.query.weight: True\n",
      "roberta.encoder.layer.3.attention.self.query.bias: True\n",
      "roberta.encoder.layer.3.attention.self.key.weight: True\n",
      "roberta.encoder.layer.3.attention.self.key.bias: True\n",
      "roberta.encoder.layer.3.attention.self.value.weight: True\n",
      "roberta.encoder.layer.3.attention.self.value.bias: True\n",
      "roberta.encoder.layer.3.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.3.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.3.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.3.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.3.output.dense.weight: True\n",
      "roberta.encoder.layer.3.output.dense.bias: True\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.4.attention.self.query.weight: True\n",
      "roberta.encoder.layer.4.attention.self.query.bias: True\n",
      "roberta.encoder.layer.4.attention.self.key.weight: True\n",
      "roberta.encoder.layer.4.attention.self.key.bias: True\n",
      "roberta.encoder.layer.4.attention.self.value.weight: True\n",
      "roberta.encoder.layer.4.attention.self.value.bias: True\n",
      "roberta.encoder.layer.4.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.4.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.4.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.4.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.4.output.dense.weight: True\n",
      "roberta.encoder.layer.4.output.dense.bias: True\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.5.attention.self.query.weight: True\n",
      "roberta.encoder.layer.5.attention.self.query.bias: True\n",
      "roberta.encoder.layer.5.attention.self.key.weight: True\n",
      "roberta.encoder.layer.5.attention.self.key.bias: True\n",
      "roberta.encoder.layer.5.attention.self.value.weight: True\n",
      "roberta.encoder.layer.5.attention.self.value.bias: True\n",
      "roberta.encoder.layer.5.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.5.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.5.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.5.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.5.output.dense.weight: True\n",
      "roberta.encoder.layer.5.output.dense.bias: True\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.6.attention.self.query.weight: True\n",
      "roberta.encoder.layer.6.attention.self.query.bias: True\n",
      "roberta.encoder.layer.6.attention.self.key.weight: True\n",
      "roberta.encoder.layer.6.attention.self.key.bias: True\n",
      "roberta.encoder.layer.6.attention.self.value.weight: True\n",
      "roberta.encoder.layer.6.attention.self.value.bias: True\n",
      "roberta.encoder.layer.6.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.6.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.6.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.6.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.6.output.dense.weight: True\n",
      "roberta.encoder.layer.6.output.dense.bias: True\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.7.attention.self.query.weight: True\n",
      "roberta.encoder.layer.7.attention.self.query.bias: True\n",
      "roberta.encoder.layer.7.attention.self.key.weight: True\n",
      "roberta.encoder.layer.7.attention.self.key.bias: True\n",
      "roberta.encoder.layer.7.attention.self.value.weight: True\n",
      "roberta.encoder.layer.7.attention.self.value.bias: True\n",
      "roberta.encoder.layer.7.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.7.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.7.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.7.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.7.output.dense.weight: True\n",
      "roberta.encoder.layer.7.output.dense.bias: True\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.8.attention.self.query.weight: True\n",
      "roberta.encoder.layer.8.attention.self.query.bias: True\n",
      "roberta.encoder.layer.8.attention.self.key.weight: True\n",
      "roberta.encoder.layer.8.attention.self.key.bias: True\n",
      "roberta.encoder.layer.8.attention.self.value.weight: True\n",
      "roberta.encoder.layer.8.attention.self.value.bias: True\n",
      "roberta.encoder.layer.8.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.8.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.8.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.8.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.8.output.dense.weight: True\n",
      "roberta.encoder.layer.8.output.dense.bias: True\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.9.attention.self.query.weight: True\n",
      "roberta.encoder.layer.9.attention.self.query.bias: True\n",
      "roberta.encoder.layer.9.attention.self.key.weight: True\n",
      "roberta.encoder.layer.9.attention.self.key.bias: True\n",
      "roberta.encoder.layer.9.attention.self.value.weight: True\n",
      "roberta.encoder.layer.9.attention.self.value.bias: True\n",
      "roberta.encoder.layer.9.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.9.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.9.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.9.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.9.output.dense.weight: True\n",
      "roberta.encoder.layer.9.output.dense.bias: True\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.10.attention.self.query.weight: True\n",
      "roberta.encoder.layer.10.attention.self.query.bias: True\n",
      "roberta.encoder.layer.10.attention.self.key.weight: True\n",
      "roberta.encoder.layer.10.attention.self.key.bias: True\n",
      "roberta.encoder.layer.10.attention.self.value.weight: True\n",
      "roberta.encoder.layer.10.attention.self.value.bias: True\n",
      "roberta.encoder.layer.10.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.10.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.10.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.10.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.10.output.dense.weight: True\n",
      "roberta.encoder.layer.10.output.dense.bias: True\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.11.attention.self.query.weight: True\n",
      "roberta.encoder.layer.11.attention.self.query.bias: True\n",
      "roberta.encoder.layer.11.attention.self.key.weight: True\n",
      "roberta.encoder.layer.11.attention.self.key.bias: True\n",
      "roberta.encoder.layer.11.attention.self.value.weight: True\n",
      "roberta.encoder.layer.11.attention.self.value.bias: True\n",
      "roberta.encoder.layer.11.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.11.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.11.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.11.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.11.output.dense.weight: True\n",
      "roberta.encoder.layer.11.output.dense.bias: True\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias: True\n",
      "lm_head.bias: True\n",
      "lm_head.dense.weight: True\n",
      "lm_head.dense.bias: True\n",
      "lm_head.layer_norm.weight: True\n",
      "lm_head.layer_norm.bias: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240517_063937-lvn6uz0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/coles/glyph_encoder/runs/lvn6uz0u' target=\"_blank\">icy-firefly-144</a></strong> to <a href='https://wandb.ai/coles/glyph_encoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/coles/glyph_encoder' target=\"_blank\">https://wandb.ai/coles/glyph_encoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/coles/glyph_encoder/runs/lvn6uz0u' target=\"_blank\">https://wandb.ai/coles/glyph_encoder/runs/lvn6uz0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1312' max='1635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1312/1635 1:14:38 < 18:24, 0.29 it/s, Epoch 4.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.148100</td>\n",
       "      <td>1.560775</td>\n",
       "      <td>0.654318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.121600</td>\n",
       "      <td>1.557898</td>\n",
       "      <td>0.656331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.104900</td>\n",
       "      <td>1.584578</td>\n",
       "      <td>0.653005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.014900</td>\n",
       "      <td>1.512970</td>\n",
       "      <td>0.662776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.952400</td>\n",
       "      <td>1.484140</td>\n",
       "      <td>0.668525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.831000</td>\n",
       "      <td>1.412836</td>\n",
       "      <td>0.678291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.792000</td>\n",
       "      <td>1.363372</td>\n",
       "      <td>0.691120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.727200</td>\n",
       "      <td>1.331165</td>\n",
       "      <td>0.695143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.684700</td>\n",
       "      <td>1.343618</td>\n",
       "      <td>0.693680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.675300</td>\n",
       "      <td>1.291191</td>\n",
       "      <td>0.704911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.636500</td>\n",
       "      <td>1.269740</td>\n",
       "      <td>0.705912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.601400</td>\n",
       "      <td>1.234692</td>\n",
       "      <td>0.712317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.599100</td>\n",
       "      <td>1.222009</td>\n",
       "      <td>0.715194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [1e-4]\n",
    "attn_dropout_probs = [0.1]\n",
    "hidden_dropout_probs = [0.1]\n",
    "mlm_probs = [0.05]\n",
    "num_epochs = 5\n",
    "\n",
    "#model_id = \"FacebookAI/xlm-roberta-base\"\n",
    "model_id = \"/models/base_2_all_layers/best_model\"\n",
    "\n",
    "run_name = \"base_2_all_layers_2\"\n",
    "\n",
    "warmup_steps = 300\n",
    "\n",
    "for lr in lrs:\n",
    "    for attn_dropout_prob in attn_dropout_probs:\n",
    "        for hidden_dropout_prob in hidden_dropout_probs:\n",
    "            for mlm_prob in mlm_probs:\n",
    "                model = train(\n",
    "                    #run_name=f\"lr={lr}-attn_do={attn_dropout_prob}-hidden_do={hidden_dropout_prob}-mlm_prob={mlm_prob}\",\n",
    "                    model_id=model_id,\n",
    "                    run_name=run_name,\n",
    "                    attn_dropout_prob=attn_dropout_prob,\n",
    "                    hidden_dropout_prob=hidden_dropout_prob,\n",
    "                    lr=lr,\n",
    "                    mlm_prob=mlm_prob,\n",
    "                    num_epochs=num_epochs,\n",
    "                    warmup_steps=warmup_steps,\n",
    "                    train_batch_size=1024,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49fa4cd3-851e-4a36-98fd-1eec2e7b4490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636cdfd4092b48e283b90fbc5d67de41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/colesimmons/xlm-r-sumerian-glyphs/commit/5a673dba8ea7af28fe93335e7db78ea5c4239dcc', commit_message='Upload XLMRobertaForMaskedLM', commit_description='', oid='5a673dba8ea7af28fe93335e7db78ea5c4239dcc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"xlm-r-sumerian-glyphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da11c92-8f40-4863-82e8-0efe0467a22c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223831a8-3355-4ac7-8f97-9cb4a5240f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    " model = AutoModelForMaskedLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00a9a9a9-d7b5-4a6c-af98-d279553acac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForMaskedLM(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(640, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): XLMRobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=640, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLMRobertaForMaskedLM.from_pretrained(\"./results/last_11_layers/best_model\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f59a8f7-9e72-4981-a6c0-52df7e48eb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497e59377bc8481fb8e3dac92c775da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "def617af-2cb0-4530-bd5c-32f61159edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the directory to delete\n",
    "directory_path = 'wandb'\n",
    "\n",
    "# Remove the directory and all its contents\n",
    "shutil.rmtree(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42151093-4519-416e-a718-9f8290aa68c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  ./results/new_tokenizer/best_model\n",
      "roberta.embeddings.word_embeddings.weight: True\n",
      "roberta.embeddings.position_embeddings.weight: True\n",
      "roberta.embeddings.token_type_embeddings.weight: True\n",
      "roberta.embeddings.LayerNorm.weight: True\n",
      "roberta.embeddings.LayerNorm.bias: True\n",
      "roberta.encoder.layer.0.attention.self.query.weight: False\n",
      "roberta.encoder.layer.0.attention.self.query.bias: False\n",
      "roberta.encoder.layer.0.attention.self.key.weight: False\n",
      "roberta.encoder.layer.0.attention.self.key.bias: False\n",
      "roberta.encoder.layer.0.attention.self.value.weight: False\n",
      "roberta.encoder.layer.0.attention.self.value.bias: False\n",
      "roberta.encoder.layer.0.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.0.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.0.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.0.output.dense.weight: False\n",
      "roberta.encoder.layer.0.output.dense.bias: False\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.1.attention.self.query.weight: False\n",
      "roberta.encoder.layer.1.attention.self.query.bias: False\n",
      "roberta.encoder.layer.1.attention.self.key.weight: False\n",
      "roberta.encoder.layer.1.attention.self.key.bias: False\n",
      "roberta.encoder.layer.1.attention.self.value.weight: False\n",
      "roberta.encoder.layer.1.attention.self.value.bias: False\n",
      "roberta.encoder.layer.1.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.1.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.1.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.1.output.dense.weight: False\n",
      "roberta.encoder.layer.1.output.dense.bias: False\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.2.attention.self.query.weight: False\n",
      "roberta.encoder.layer.2.attention.self.query.bias: False\n",
      "roberta.encoder.layer.2.attention.self.key.weight: False\n",
      "roberta.encoder.layer.2.attention.self.key.bias: False\n",
      "roberta.encoder.layer.2.attention.self.value.weight: False\n",
      "roberta.encoder.layer.2.attention.self.value.bias: False\n",
      "roberta.encoder.layer.2.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.2.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.2.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.2.output.dense.weight: False\n",
      "roberta.encoder.layer.2.output.dense.bias: False\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.3.attention.self.query.weight: False\n",
      "roberta.encoder.layer.3.attention.self.query.bias: False\n",
      "roberta.encoder.layer.3.attention.self.key.weight: False\n",
      "roberta.encoder.layer.3.attention.self.key.bias: False\n",
      "roberta.encoder.layer.3.attention.self.value.weight: False\n",
      "roberta.encoder.layer.3.attention.self.value.bias: False\n",
      "roberta.encoder.layer.3.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.3.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.3.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.3.output.dense.weight: False\n",
      "roberta.encoder.layer.3.output.dense.bias: False\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.4.attention.self.query.weight: False\n",
      "roberta.encoder.layer.4.attention.self.query.bias: False\n",
      "roberta.encoder.layer.4.attention.self.key.weight: False\n",
      "roberta.encoder.layer.4.attention.self.key.bias: False\n",
      "roberta.encoder.layer.4.attention.self.value.weight: False\n",
      "roberta.encoder.layer.4.attention.self.value.bias: False\n",
      "roberta.encoder.layer.4.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.4.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.4.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.4.output.dense.weight: False\n",
      "roberta.encoder.layer.4.output.dense.bias: False\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.5.attention.self.query.weight: False\n",
      "roberta.encoder.layer.5.attention.self.query.bias: False\n",
      "roberta.encoder.layer.5.attention.self.key.weight: False\n",
      "roberta.encoder.layer.5.attention.self.key.bias: False\n",
      "roberta.encoder.layer.5.attention.self.value.weight: False\n",
      "roberta.encoder.layer.5.attention.self.value.bias: False\n",
      "roberta.encoder.layer.5.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.5.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.5.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.5.output.dense.weight: False\n",
      "roberta.encoder.layer.5.output.dense.bias: False\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.6.attention.self.query.weight: False\n",
      "roberta.encoder.layer.6.attention.self.query.bias: False\n",
      "roberta.encoder.layer.6.attention.self.key.weight: False\n",
      "roberta.encoder.layer.6.attention.self.key.bias: False\n",
      "roberta.encoder.layer.6.attention.self.value.weight: False\n",
      "roberta.encoder.layer.6.attention.self.value.bias: False\n",
      "roberta.encoder.layer.6.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.6.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.6.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.6.output.dense.weight: False\n",
      "roberta.encoder.layer.6.output.dense.bias: False\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.7.attention.self.query.weight: False\n",
      "roberta.encoder.layer.7.attention.self.query.bias: False\n",
      "roberta.encoder.layer.7.attention.self.key.weight: False\n",
      "roberta.encoder.layer.7.attention.self.key.bias: False\n",
      "roberta.encoder.layer.7.attention.self.value.weight: False\n",
      "roberta.encoder.layer.7.attention.self.value.bias: False\n",
      "roberta.encoder.layer.7.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.7.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.7.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.7.output.dense.weight: False\n",
      "roberta.encoder.layer.7.output.dense.bias: False\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.8.attention.self.query.weight: False\n",
      "roberta.encoder.layer.8.attention.self.query.bias: False\n",
      "roberta.encoder.layer.8.attention.self.key.weight: False\n",
      "roberta.encoder.layer.8.attention.self.key.bias: False\n",
      "roberta.encoder.layer.8.attention.self.value.weight: False\n",
      "roberta.encoder.layer.8.attention.self.value.bias: False\n",
      "roberta.encoder.layer.8.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.8.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.8.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.8.output.dense.weight: False\n",
      "roberta.encoder.layer.8.output.dense.bias: False\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.9.attention.self.query.weight: True\n",
      "roberta.encoder.layer.9.attention.self.query.bias: True\n",
      "roberta.encoder.layer.9.attention.self.key.weight: True\n",
      "roberta.encoder.layer.9.attention.self.key.bias: True\n",
      "roberta.encoder.layer.9.attention.self.value.weight: True\n",
      "roberta.encoder.layer.9.attention.self.value.bias: True\n",
      "roberta.encoder.layer.9.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.9.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.9.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.9.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.9.output.dense.weight: True\n",
      "roberta.encoder.layer.9.output.dense.bias: True\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.10.attention.self.query.weight: True\n",
      "roberta.encoder.layer.10.attention.self.query.bias: True\n",
      "roberta.encoder.layer.10.attention.self.key.weight: True\n",
      "roberta.encoder.layer.10.attention.self.key.bias: True\n",
      "roberta.encoder.layer.10.attention.self.value.weight: True\n",
      "roberta.encoder.layer.10.attention.self.value.bias: True\n",
      "roberta.encoder.layer.10.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.10.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.10.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.10.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.10.output.dense.weight: True\n",
      "roberta.encoder.layer.10.output.dense.bias: True\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.11.attention.self.query.weight: True\n",
      "roberta.encoder.layer.11.attention.self.query.bias: True\n",
      "roberta.encoder.layer.11.attention.self.key.weight: True\n",
      "roberta.encoder.layer.11.attention.self.key.bias: True\n",
      "roberta.encoder.layer.11.attention.self.value.weight: True\n",
      "roberta.encoder.layer.11.attention.self.value.bias: True\n",
      "roberta.encoder.layer.11.attention.output.dense.weight: True\n",
      "roberta.encoder.layer.11.attention.output.dense.bias: True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "roberta.encoder.layer.11.intermediate.dense.weight: True\n",
      "roberta.encoder.layer.11.intermediate.dense.bias: True\n",
      "roberta.encoder.layer.11.output.dense.weight: True\n",
      "roberta.encoder.layer.11.output.dense.bias: True\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight: True\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias: True\n",
      "lm_head.bias: True\n",
      "lm_head.dense.weight: True\n",
      "lm_head.dense.bias: True\n",
      "lm_head.layer_norm.weight: True\n",
      "lm_head.layer_norm.bias: True\n"
     ]
    }
   ],
   "source": [
    "model = XLMRobertaForMaskedLM.from_pretrained(\n",
    "        model_id,\n",
    "        attention_probs_dropout_prob=attn_dropout_prob,\n",
    "        hidden_dropout_prob=hidden_dropout_prob,\n",
    "        #num_hidden_layers=num_hidden_layers,\n",
    "        #num_attention_heads=num_attention_heads,\n",
    "    )\n",
    "print(\"Loading: \", model_id)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer.\" in name and int(name.split(\".\")[3]) >= 8:\n",
    "        param.requires_grad = True\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "# Freeze all layers in the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d2272-ed26-432b-b28c-a473831dcb82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ff7f63-cfda-41cb-9c71-499091758774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers, processors\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d438976a-8b74-4037-88b3-c4cd7e2e07ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'P119622', 'period': 'Ur III', 'genre': 'Administrative', 'transliteration': '<SURFACE>\\n2(u) udu\\nki nin-Å¡eâ‚ƒ\\nki uÅ¡-mu-ta\\nkiÅ¡ibâ‚ƒ ensiâ‚‚-ka\\n<SURFACE>\\nÅ¡aâ‚ƒ uriâ‚…{ki}ma\\niti ezem-mah\\nmu en eridu{ki} ba-hun\\n<SURFACE>\\n<COLUMN>\\n{d}amar{d}suen\\nlugal kal-ga\\nlugal uriâ‚…{ki}ma\\nlugal an ub-da limmuâ‚‚-ba\\n<COLUMN>\\na-kal-la\\nensiâ‚‚\\numma{ki}\\n<unk> zu', 'glyph_names': '<SURFACE> \\n |U.U| LU \\n KI |SAL.TUGâ‚‚| EÅ â‚‚ \\n KI UÅ  MU TA \\n DUB |PA.TE.SI| KA \\n <SURFACE> \\n Å Aâ‚ƒ |Å EÅ .AB| KI MA \\n |UDÃ—(U.U.U)| EZEN MAH \\n MU EN NUN KI BA EÅ â‚‚ \\n <SURFACE> \\n <COLUMN> \\n AN AMAR AN |EN.ZU| \\n LUGAL KAL GA \\n LUGAL |Å EÅ .AB| KI MA \\n LUGAL AN UB DA LIMMUâ‚‚ BA \\n <COLUMN> \\n A KAL LA \\n |PA.TE.SI| \\n |GIÅ .KUÅ Uâ‚‚| KI \\n <unk> ZU', 'glyphs': '<SURFACE>\\nğ’™ğ’‡»\\nğ’† ğ’ğ’‚ \\nğ’† ğ’‘ğ’ˆ¬ğ’‹«\\nğ’¾ğ’‰ºğ’‹¼ğ’‹›ğ’…—\\n<SURFACE>\\nğ’Š®ğ’‹€ğ’€Šğ’† ğ’ˆ \\nğ’Œ—ğ’‚¡ğ’ˆ¤\\nğ’ˆ¬ğ’‚—ğ’‰£ğ’† ğ’€ğ’‚ \\n<SURFACE>\\n<COLUMN>\\nğ’€­ğ’€«ğ’€­ğ’‚—ğ’ª\\nğ’ˆ—ğ’†—ğ’‚µ\\nğ’ˆ—ğ’‹€ğ’€Šğ’† ğ’ˆ \\nğ’ˆ—ğ’€­ğ’Œ’ğ’•ğ’‡¹ğ’€\\n<COLUMN>\\nğ’€€ğ’†—ğ’†·\\nğ’‰ºğ’‹¼ğ’‹›\\nğ’„‘ğ’†µğ’† \\n<unk>ğ’ª'}\n",
      "ğ’™ğ’‡»ğ’† ğ’ğ’‚ ğ’† ğ’‘ğ’ˆ¬ğ’‹«ğ’¾ğ’‰ºğ’‹¼ğ’‹›ğ’…—ğ’Š®ğ’‹€ğ’€Šğ’† ğ’ˆ ğ’Œ—ğ’‚¡ğ’ˆ¤ğ’ˆ¬ğ’‚—ğ’‰£ğ’† ğ’€ğ’‚ ğ’€­ğ’€«ğ’€­ğ’‚—ğ’ªğ’ˆ—ğ’†—ğ’‚µğ’ˆ—ğ’‹€ğ’€Šğ’† ğ’ˆ ğ’ˆ—ğ’€­ğ’Œ’ğ’•ğ’‡¹ğ’€ğ’€€ğ’†—ğ’†·ğ’‰ºğ’‹¼ğ’‹›ğ’„‘ğ’†µğ’† ğ’ª\n"
     ]
    }
   ],
   "source": [
    "ALL_SPECIAL_TOKS = [\n",
    "    '<s>',\n",
    "    '<pad>',\n",
    "    '</s>',\n",
    "    '<unk>',\n",
    "    '<mask>',\n",
    "    \"\\n\",\n",
    "    \"<SURFACE>\",\n",
    "    \"<COLUMN>\",\n",
    "    \"<BLANK_SPACE>\",\n",
    "    \"<RULING>\",\n",
    "    \"...\"\n",
    "]                        \n",
    "\n",
    "def get_glyph_texts():\n",
    "    glyph_texts = []\n",
    "    for split in dataset.keys():\n",
    "        for example in dataset[split]:\n",
    "            text = example[\"glyphs\"]\n",
    "            for token_ in ALL_SPECIAL_TOKS:\n",
    "                text = text.replace(token_, \"\")\n",
    "            glyph_texts.append(text)\n",
    "    print(glyph_texts[0])\n",
    "    return glyph_texts\n",
    "\n",
    "print(dataset[\"train\"][0])\n",
    "glyph_texts = get_glyph_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4962d2-e6c9-4e6e-af01-349894e4792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unique glyphs:  621\n",
      "{'ğ’†', 'ğ’’', 'ğ’€¶', 'ğ’‹¤', 'ğ’€‰', 'ğ’Œœ', 'ğ’‚‰', 'ğ’„¤', 'ğ’‹¡', 'ğ’Š¯', 'ğ’‰ª', 'ğ’“»', 'ğ’‚¶', 'ğ’º', 'ğ’‚', 'ğ’ˆ¨', 'ğ’ˆ¤', 'ğ’Œº', 'ğ’‚¢', 'ğ’‡‘', 'ğ’…˜', 'ğ’€ª', 'ğ’•', 'ğ’„†', 'ğ’…¥', 'ğ’¡', 'ğ’„±', 'ğ’—', 'ğ’…', '\\xa0', 'ğ’', 'ğ’°', 'ğ’‡¦', 'ğ’†‘', 'ğ’Œˆ', 'ğ’Œ†', 'ğ’ƒ', 'ğ’ƒ´', 'ğ’Š—', 'ğ’„ª', 'ğ’¯', 'ğ’€', 'ğ’ˆ¬', 'ğ’Œ¢', 'ğ’‚ƒ', 'ğ’‹', 'ğ’Œ', 'ğ’‰¢', 'ğ’', 'ğ’•', 'ğ’‡»', 'ğ’†ˆ', 'ğ’…Œ', 'ğ’‡½', 'ğ’¨', 'ğ’‚€', 'ğ’‹€', 'ğ’‹¸', 'ğ’„–', 'ğ’‰ˆ', 'ğ’‚•', 'ğ’‰½', 'ğ’Š¨', 'ğ’‹', 'ğ’‹„', 'ğ’»', 'ğ’Œ‹', 'ğ’‹ƒ', 'ğ’‘™', 'ğ’‚·', 'ğ’™', 'ğ’†š', 'ğ’‹', 'ğ’’¬', 'ğ’›', 'ğ’ˆ½', 'ğ’ˆ£', 'ğ’‡²', 'ğ’‰˜', 'ğ’‡¿', 'ğ’‡¡', 'ğ’ƒ˜', 'ğ’‡', 'ğ’ƒ¶', 'ğ’¦', 'ğ’Š²', 'ğ’Š­', 'ğ’†¶', 'ğ’ƒ', 'ğ’‡', 'ğ’†€', 'ğ’š', 'ğ’„½', 'ğ’¼', 'ğ’†¢', 'ğ’ˆ¸', 'ğ’Š«', 'ğ’ƒ±', 'ğ’‰€', 'ğ’ƒ£', 'ğ’ƒ•', 'ğ’‹™', 'ğ’', 'ğ’“¼', 'ğ’€²', 'ğ’ˆ', 'ğ’„®', 'ğ’‚', 'ğ’…', 'ğ’œ', 'ğ’€‡', 'ğ’‡š', 'ğ’–', 'ğ’‚£', 'ğ’°', 'ğ’€¸', 'ğ’©', 'ğ’ˆ', 'ğ’®', 'ğ’ƒ°', 'ğ’€‹', 'ğ’¦', 'ğ’', 'ğ’ƒŸ', 'ğ’†¤', 'ğ’—', 'ğ’ƒ ', 'ğ’Š', 'ğ’€', 'ğ’€³', 'ğ’', 'ğ’ˆ²', 'ğ’‡µ', 'ğ’„¢', 'ğ’½', 'ğ’Œ…', 'ğ’‹º', 'ğ’Œ¦', 'ğ’”¸', 'ğ’’', 'ğ’¶', 'ğ’ˆ¥', 'ğ’‡º', 'ğ’„§', 'ğ’ˆ­', 'ğ’†³', 'ğ’‹ ', 'ğ’ƒ¹', 'ğ’„', 'ğ’‘', 'ğ’Š¢', 'ğ’', 'ğ’Š ', 'ğ’‚¯', 'ğ’‹°', 'ğ’…œ', 'ğ’‹­', 'ğ’„´', 'ğ’¢', 'ğ’…', 'ğ’‰º', 'ğ’†°', 'ğ’‹§', 'ğ’…–', 'ğ’‘˜', 'ğ’”', 'ğ’ª', 'ğ’€¯', 'ğ’‘–', 'ğ’„¸', 'ğ’‡¯', 'ğ’€', 'ğ’Š¾', 'ğ’', 'ğ’‡‡', 'ğ’Š¿', 'ğ’‡³', 'ğ’†', 'ğ’‡‹', 'ğ’‚…', 'ğ’ƒ·', 'ğ’…¾', 'ğ’‡¼', 'ğ’ƒ‹', 'ğ’‰š', 'ğ’„˜', 'ğ’€œ', 'ğ’Š½', 'ğ’…“', 'ğ’„¨', 'ğ’‚Ÿ', 'ğ’†', 'ğ’…—', 'ğ’ƒ½', 'ğ’Š', 'ğ’”', 'ğ’„', 'ğ’€µ', 'ğ’‰¦', 'ğ’±', 'ğ’š', 'ğ’…¿', 'ğ’†œ', 'ğ’†˜', 'ğ’„Ÿ', 'ğ’†¥', 'ğ’……', 'ğ’Œ', 'ğ’Š‘', 'ğ’‹¨', 'ğ’©', 'ğ’·', 'ğ’Šš', 'ğ’€˜', 'ğ’‘—', 'ğ’ˆ¯', 'ğ’’', 'ğ’ƒ¸', 'ğ’‚–', 'ğ’Œ™', 'ğ’…', 'ğ’…»', 'ğ’¤', 'ğ’Š', 'ğ’ƒ', 'ğ’¹', 'ğ’‡‰', 'ğ’€€', 'ğ’…ˆ', 'ğ’ˆ§', 'ğ’‚—', 'ğ’ˆ—', 'ğ’ŠŠ', 'ğ’¬', 'ğ’º', 'ğ’„·', 'ğ’†’', 'ğ’Œ§', 'ğ’…¡', 'ğ’…ƒ', 'ğ’‘', 'ğ’‚', 'ğ’Œ‰', 'ğ’Œ´', 'ğ’†·', 'ğ’„­', 'ğ’†¬', 'ğ’†', 'ğ’‚«', 'ğ’‡…', 'ğ’„', 'ğ’‚¼', 'ğ’‚š', 'ğ’‡', 'ğ’€', 'ğ’ƒ²', 'ğ’µ', 'ğ’“ˆ', 'ğ’ˆ', 'ğ’„°', 'ğ’…‡', 'ğ’ˆœ', 'ğ’†¾', 'ğ’“', 'ğ’‡¥', 'ğ’…‹', 'ğ’Š’', 'ğ’‰’', 'ğ’‰¼', 'ğ’‹', 'ğ’Œª', 'ğ’ˆª', 'ğ’‘', 'ğ’„«', 'ğ’‰', 'ğ’£', 'ğ’‰´', 'ğ’„’', 'ğ’‹†', 'ğ’Œ¿', 'ğ’ƒ‰', 'ğ’Œ‘', 'ğ’ ', 'ğ’Œ', 'ğ’‘œ', 'ğ’½', 'ğ’ƒ', 'ğ’“', 'ğ’‡', 'ğ’‘„', 'ğ’¤', 'ğ’†µ', 'ğ’…²', 'ğ’Œ«', 'ğ’', 'ğ’Œ¬', 'ğ’‰Œ', 'ğ’€©', 'ğ’†“', 'ğ’‰©', 'ğ’‡±', 'ğ’¼', 'ğ’²', 'ğ’‰“', 'ğ’ŠŒ', 'ğ’‹¼', 'ğ’ƒŒ', 'ğ’†§', 'ğ’¶', 'ğ’Š·', 'ğ’•', 'ğ’‰', 'ğ’ƒº', 'ğ’‹—', 'ğ’', 'ğ’›', 'ğ’‡†', 'ğ’‹‹', 'ğ’‰£', 'ğ’ƒ¿', 'ğ’Œ¸', 'ğ’–', 'ğ’„‰', 'ğ’‘†', 'ğ’‚ ', 'ğ’€”', 'ğ’€–', 'ğ’€', 'ğ’…', 'ğ’ˆ±', 'ğ’‚‚', 'ğ’€¿', 'ğ’‰', 'ğ’ˆ', 'ğ’³', 'ğ’ˆ¾', 'ğ’‰»', 'ğ’‚', 'ğ’Œš', 'ğ’ˆ‚', 'ğ’€Š', 'ğ’„‘', 'ğ’€®', 'ğ’Œ', 'ğ’‹¥', 'ğ’Œ', 'ğ’ƒ¢', 'ğ’ˆ¹', 'ğ’„¬', 'ğ’„‹', 'ğ’ˆ¿', 'ğ’‚„', 'ğ’†', 'ğ’‰­', 'ğ’‰¿', 'ğ’‚ˆ', 'ğ’ˆ€', 'ğ’‹’', 'ğ’†ª', 'ğ’†²', 'ğ’„€', 'ğ’¹', 'ğ’Œ”', 'ğ’‡­', 'ğ’€ ', 'ğ’‚¦', 'ğ’…º', 'ğ’Œƒ', 'ğ’Š', 'ğ’“', 'ğ’Š©', 'ğ’Šº', 'ğ’€•', 'ğ’ˆ®', 'ğ’ŒŒ', 'ğ’‰…', 'ğ’‚¥', 'ğ’Ÿ', 'ğ’„¦', 'ğ’‘”', 'ğ’‚', 'ğ’Œ’', 'ğ’†ƒ', 'ğ’‹»', 'ğ’’¾', 'ğ’', 'ğ’‡’', 'ğ’‡§', 'ğ’‚¨', 'ğ’…¢', 'ğ’ˆ¦', 'ğ’¼', 'ğ’„©', 'ğ’­', 'ğ’ˆ©', 'ğ’ª', 'ğ’†', 'ğ’Œ£', 'ğ’œ', 'ğ’€«', 'ğ’‹³', 'ğ’„¾', 'ğ’†¯', 'ğ’‡Ÿ', 'ğ’', 'ğ’‚µ', 'ğ’…´', 'ğ’„Œ', 'ğ’„Š', 'ğ’†‰', 'ğ’‹', 'ğ’†¹', 'ğ’®', 'ğ’Œ¾', 'ğ’„£', 'ğ’‰†', 'ğ’‹·', 'ğ’‡¸', 'ğ’‰¥', 'ğ’‘ ', 'ğ’€„', 'ğ’…®', 'ğ’', 'ğ’Š»', 'ğ’¦', 'ğ’‹«', 'ğ’', 'ğ’‹¾', 'ğ’„¥', 'ğ’Š¬', 'ğ’Œ', 'ğ’‹', 'ğ’„', 'ğ’„›', 'ğ’§', 'ğ’„—', 'ğ’', 'ğ’…”', 'ğ’ˆ•', 'ğ’Š®', 'ğ’†¦', 'ğ’ƒ', 'ğ’‡™', 'ğ’Š”', 'ğ’‚Š', 'ğ’ƒ¾', 'ğ’Œ“', 'ğ’‚™', 'ğ’‹“', 'ğ’‘', 'ğ’Œ±', 'ğ’‚°', 'ğ’‡´', 'ğ’Ÿ', 'ğ’‚¸', 'ğ’‹›', 'ğ’“º', 'ğ’Œ€', 'ğ’‘’', 'ğ’‰', 'ğ’­', 'ğ’†¨', 'ğ’„ƒ', 'ğ’ƒ«', 'ğ’±', 'ğ’‰', 'ğ’Œ•', 'ğ’‡ˆ', 'ğ’‚‡', 'ğ’¡', 'ğ’Œ„', 'ğ’Œ›', 'ğ’ƒ¡', 'ğ’‹–', 'ğ’„‡', 'ğ’†—', 'ğ’Š¹', 'ğ’‡¹', 'ğ’ˆ ', 'ğ’‹¢', 'ğ’†­', 'ğ’ˆ›', 'ğ’¢', 'ğ’‚®', 'ğ’™', 'ğ’†¸', 'ğ’ƒ³', 'ğ’…¤', 'ğ’’', 'ğ’«', 'ğ’‹ª', 'ğ’Š', 'ğ’„¯', 'ğ’ƒ…', 'ğ’„•', 'ğ’„²', 'ğ’¾', 'ğ’‰„', 'ğ’„', 'ğ’‹½', 'ğ’‹', 'ğ’”²', 'ğ’Œµ', 'ğ’‡·', 'ğ’ˆ¢', 'ğ’–', 'ğ’‰ ', 'ğ’‹‡', 'ğ’‡°', 'ğ’€§', 'ğ’…Š', 'ğ’ ', 'ğ’ƒ©', 'ğ’˜', 'ğ’´', 'ğ’', 'ğ’†‚', 'ğ’ˆ–', 'ğ’„', 'ğ’€›', 'ğ’ƒ»', 'ğ’”', 'ğ’†›', 'ğ’‹¦', 'ğ’‡€', 'ğ’‰', 'ğ’‡Œ', 'ğ’‰µ', 'ğ’„µ', 'ğ’•‚', 'ğ’¨', 'ğ’‰', 'ğ’ˆ°', 'ğ’Š´', 'ğ’Š“', 'ğ’‡¬', 'ğ’‰‹', 'ğ’‚¬', 'ğ’† ', 'ğ’‹œ', 'ğ’‚”', 'ğ’‹š', 'ğ’€', 'ğ’‰–', 'ğ’€š', 'ğ’‰¯', 'ğ’š', 'ğ’„„', 'ğ’¸', 'ğ’‘Ÿ', 'ğ’‚†', 'ğ’µ', 'ğ’Š™', 'ğ’€', 'ğ’‰®', 'ğ’€¼', 'ğ’ª', 'ğ’Œ·', 'ğ’€­', 'ğ’Œ¤', 'ğ’Œ¨', 'ğ’ƒ®', 'ğ’Œ', 'ğ’ˆ', 'ğ’€¹', 'ğ’„™', 'ğ’‘š', 'ğ’„ˆ', 'ğ’‘', 'ğ’…‚', 'ğ’‰‡', 'ğ’¬', 'ğ’†•', 'ğ’†', 'ğ’ƒµ', 'ğ’®', 'ğ’¥', 'ğ’‘', 'ğ’€¬', 'ğ’£', 'ğ’„¿', 'ğ’‚¤', 'ğ’', 'ğ’„ ', 'ğ’¬', 'ğ’‹©', 'ğ’ƒ™', 'ğ’Š•', 'ğ’‘‘', 'ğ’´', 'ğ’Œ²', 'ğ’Œ¶', 'ğ’£', 'ğ’‘›', 'ğ’†Ÿ', 'ğ’Œ—', 'ğ’¥', 'ğ’', 'ğ’‰¾', 'ğ’‡¶', 'ğ’…', 'ğ’€¾', 'ğ’Œ‡', 'ğ’ƒ¼', 'ğ’‚¡', 'ğ’Œ°', 'ğ’ˆ«', 'ğ’ˆ»', 'ğ’…•', 'ğ’ƒ¥', 'ğ’‹', 'ğ’Œ¼', 'ğ’…†', 'ğ’', 'ğ’«', 'ğ’‰¡', 'ğ’‰¶', 'ğ’•', 'ğ’€´', 'ğ’…€', 'ğ’ˆŒ'}\n"
     ]
    }
   ],
   "source": [
    "def get_unique_glyphs(dataset):\n",
    "    unique_glyphs = set()\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        for example in dataset[split]:\n",
    "            text = example[\"glyphs\"]\n",
    "            for token_ in ALL_SPECIAL_TOKS:\n",
    "                text = text.replace(token_, \"\")\n",
    "            unique_glyphs |= set(text)\n",
    "            \n",
    "    print(\"Num unique glyphs: \", len(unique_glyphs))\n",
    "    print(unique_glyphs)\n",
    "    return unique_glyphs\n",
    "\n",
    "unique_glyphs = get_unique_glyphs(dataset)\n",
    "unique_glyph_count = len(unique_glyphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "28f242e2-6903-41ee-88bf-a5c820cd180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'0-64': 70212, '64-128': 20260, 'mt256': 4240, '128-256': 7412})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_glyph_lens(dataset):\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        for example in dataset[split]:\n",
    "            text = example[\"glyphs\"]\n",
    "            for token_ in ALL_SPECIAL_TOKS:\n",
    "                text = text.replace(token_, \"\")\n",
    "            if len(text) <= 64:\n",
    "                counts[\"0-64\"] += 1\n",
    "            elif len(text) <= 128:\n",
    "                counts[\"64-128\"] += 1\n",
    "            elif len(text) <= 256:\n",
    "                counts[\"128-256\"] += 1\n",
    "            else:\n",
    "                counts[\"mt256\"] += 1\n",
    "    return counts\n",
    "\n",
    "get_glyph_lens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0b67682-8e54-4246-ad9e-97664ae3b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(pre_tokenizers.ByteLevel.alphabet()) + unique_glyph_count + len(list(ALL_SPECIAL_TOKS))\n",
    "unique_glyph_count + len(list(ALL_SPECIAL_TOKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "061c6f1e-0a4d-4a45-80e0-af0bbb27baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ALL_SPECIAL_TOKS = [\n",
    "    '<s>',\n",
    "    '<pad>',\n",
    "    '</s>',\n",
    "    '<unk>',\n",
    "    '<mask>',\n",
    "    \"\\n\",\n",
    "    \"<SURFACE>\",\n",
    "    \"<COLUMN>\",\n",
    "    \"<BLANK_SPACE>\",\n",
    "    \"<RULING>\",\n",
    "    \"...\"\n",
    "]                        \n",
    "\n",
    "def train_tokenizer():\n",
    "    #vocab_size = len(pre_tokenizers.ByteLevel.alphabet()) + unique_glyph_count + len(list(SPECIAL_TOKENS)) -> 888\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        initial_alphabet=list(unique_glyphs),\n",
    "        #min_frequency=100,\n",
    "        vocab_size=unique_glyph_count + len(list(ALL_SPECIAL_TOKS)),\n",
    "        special_tokens=list(ALL_SPECIAL_TOKS),\n",
    "        show_progress=True,\n",
    "        #max_token_length=2,\n",
    "    )\n",
    "    #tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    #tokenizer.normalizer = None\n",
    "    #tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.add_special_tokens(list(ALL_SPECIAL_TOKS))\n",
    "    #print(tokenizer.token_to_id('<s>'))\n",
    "    #tokenizer.normalizer = normalizers.NFD()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.UnicodeScripts()\n",
    "    tokenizer.post_processor = processors.RobertaProcessing(\n",
    "        (\"</s>\", tokenizer.token_to_id('</s>')),\n",
    "        (\"<s>\", tokenizer.token_to_id('<s>'))\n",
    "    )\n",
    "    tokenizer.decoder = decoders.BPEDecoder()\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator=glyph_texts, trainer=trainer)\n",
    "    tokenizer.save(\"tokenizer.json\")\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = train_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "683753c3-5c1d-4fd6-b6c4-99701a67347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "['\\n']\n",
      "622\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<mask>',\n",
       " '<unk>',\n",
       " '<s>',\n",
       " '<pad>',\n",
       " '<BLANK_SPACE>',\n",
       " '</s>',\n",
       " '<COLUMN>',\n",
       " '<SURFACE>',\n",
       " '<RULING>',\n",
       " '...']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "glyph_vocab_items = [k for k in tokenizer.get_vocab().keys() if len(k) == 1]\n",
    "print([x for x in glyph_vocab_items if x not in unique_glyphs])\n",
    "print(len(glyph_vocab_items))\n",
    "other_vocab_items = [k for k in tokenizer.get_vocab().keys() if len(k) != 1]\n",
    "print(len(other_vocab_items))\n",
    "other_vocab_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5adae3f-36b0-40c7-aede-4dc924dea324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='FacebookAI/xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "847dde0c-ccf6-4088-835c-f4ba499934cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/colesimmons/SumerianGlyphTokenizer_Roberta/commit/75fadd99143f2d2fc690c285b7228d262c524be1', commit_message='Upload tokenizer', commit_description='', oid='75fadd99143f2d2fc690c285b7228d262c524be1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<SURFACE>', '<COLUMN>', '<BLANK_SPACE>', '<RULING>', '\\n', '...']})\n",
    "tokenizer.push_to_hub(\"SumerianGlyphTokenizer_Roberta\")\n",
    "#tokenizer.save(\"encoder_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "af850f80-1791-43f5-8cb5-8525b6429565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=632, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<SURFACE>', '<COLUMN>', '<BLANK_SPACE>', '<RULING>', '\\n', '...']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<SURFACE>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<COLUMN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<BLANK_SPACE>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<RULING>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"...\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00177acb-2669-40e4-b542-2b945a787d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SURFACE>\n",
      "ğ’¹ğ’ƒ¢ğ’‰ºğ’‹¼ğ’‹›ğ’…—ğ’‰Œğ’‡»ğ’† \n",
      "ğ’¹ğ’ƒ¢ğ’†ªğ’•ğ’ˆğ’‰¡ğ’Œ‰\n",
      "ğ’¹ğ’ƒ¢ğ’¹ğ’ˆ§\n",
      "ğ’‚—ğ’€­ğ’ˆ¹\n",
      "ğ’¹ğ’ƒ¢ğ’…—ğ’‚µğ’ˆ¬\n",
      "ğ’¹ğ’„ğ’Šº\n",
      "<SURFACE>\n",
      "ğ’Šğ’‡»ğ’¹ğ’ƒ¢\n",
      "ğ’€Šğ’€ğ’†—ğ’†·ğ’ˆ¾ğ’ƒ°\n",
      "ğ’ˆ¬ğ’º\n",
      "ğ’Œ—ğ’‚¡ğ’€­ğ’ğ’€€ğ’ª\n",
      "ğ’ˆ¬ğ’‚—ğ’€­ğ’‹€ğ’† ğ’ˆ¦ğ’‚Šğ’‰Œğ’…†ğ’Š’\n",
      "<SURFACE>\n",
      "ğ’Œ“ğ’¹ğ’„°\n",
      "62\n",
      "81\n",
      "{'input_ids': [0, 6, 5, 87, 144, 389, 464, 442, 232, 366, 319, 267, 5, 87, 144, 274, 64, 333, 376, 475, 5, 87, 144, 87, 340, 5, 108, 35, 353, 5, 87, 144, 232, 126, 345, 5, 87, 187, 422, 5, 6, 5, 562, 319, 87, 144, 5, 16, 48, 260, 283, 356, 150, 5, 345, 88, 5, 485, 114, 35, 546, 12, 536, 5, 345, 108, 35, 427, 267, 339, 103, 366, 219, 402, 5, 6, 5, 482, 87, 204, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "<s><SURFACE>\n",
      "ğ’¹ğ’ƒ¢ğ’‰ºğ’‹¼ğ’‹›ğ’…—ğ’‰Œğ’‡»ğ’† \n",
      "ğ’¹ğ’ƒ¢ğ’†ªğ’•ğ’ˆğ’‰¡ğ’Œ‰\n",
      "ğ’¹ğ’ƒ¢ğ’¹ğ’ˆ§\n",
      "ğ’‚—ğ’€­ğ’ˆ¹\n",
      "ğ’¹ğ’ƒ¢ğ’…—ğ’‚µğ’ˆ¬\n",
      "ğ’¹ğ’„ğ’Šº\n",
      "<SURFACE>\n",
      "ğ’Šğ’‡»ğ’¹ğ’ƒ¢\n",
      "ğ’€Šğ’€ğ’†—ğ’†·ğ’ˆ¾ğ’ƒ°\n",
      "ğ’ˆ¬ğ’º\n",
      "ğ’Œ—ğ’‚¡ğ’€­ğ’ğ’€€ğ’ª\n",
      "ğ’ˆ¬ğ’‚—ğ’€­ğ’‹€ğ’† ğ’ˆ¦ğ’‚Šğ’‰Œğ’…†ğ’Š’\n",
      "<SURFACE>\n",
      "ğ’Œ“ğ’¹ğ’„°</s>\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0][\"glyphs\"]\n",
    "without_special = example\n",
    "for special_ in ALL_SPECIAL_TOKS:\n",
    "    without_special = without_special.replace(special_, \"\")\n",
    "print(example)\n",
    "print(len(without_special))\n",
    "encoded = tokenizer(example)\n",
    "print(len(encoded[\"input_ids\"]))\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded[\"input_ids\"]))\n",
    "print(tokenizer.get_special_tokens_mask(encoded[\"input_ids\"], already_has_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4f9b961-4193-479f-9eee-7c05fbd8a84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SURFACE>\n",
      "ğ’¹ğ’ƒ¢ğ’‰ºğ’‹¼ğ’‹›ğ’…—ğ’‰Œğ’‡»ğ’† \n",
      "ğ’¹ğ’ƒ¢ğ’†ªğ’•ğ’ˆğ’‰¡ğ’Œ‰\n",
      "ğ’¹ğ’ƒ¢ğ’¹ğ’ˆ§\n",
      "ğ’‚—ğ’€­ğ’ˆ¹\n",
      "ğ’¹ğ’ƒ¢ğ’…—ğ’‚µğ’ˆ¬\n",
      "ğ’¹ğ’„ğ’Šº\n",
      "<SURFACE>\n",
      "ğ’Šğ’‡»ğ’¹ğ’ƒ¢\n",
      "ğ’€Šğ’€ğ’†—ğ’†·ğ’ˆ¾ğ’ƒ°\n",
      "ğ’ˆ¬ğ’º\n",
      "ğ’Œ—ğ’‚¡ğ’€­ğ’ğ’€€ğ’ª\n",
      "ğ’ˆ¬ğ’‚—ğ’€­ğ’‹€ğ’† ğ’ˆ¦ğ’‚Šğ’‰Œğ’…†ğ’Š’\n",
      "<SURFACE>\n",
      "ğ’Œ“ğ’¹ğ’„°\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 6, 5, 87, 144, 389, 464, 442, 232, 366, 319, 267, 5, 87, 144, 274, 64, 333, 376, 475, 5, 87, 144, 87, 340, 5, 108, 35, 353, 5, 87, 144, 232, 126, 345, 5, 87, 187, 422, 5, 6, 5, 562, 319, 87, 144, 5, 16, 48, 260, 283, 356, 150, 5, 345, 88, 5, 485, 114, 35, 546, 12, 536, 5, 345, 108, 35, 427, 267, 339, 103, 366, 219, 402, 5, 6, 5, 482, 87, 204, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"glyphs\"])\n",
    "tokenizer(dataset[\"train\"][0][\"glyphs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0ee1bcb-2e91-4cc5-b50d-5978bbea9080",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m encoded:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoded, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3811\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3811\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3815\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "for id in encoded:\n",
    "    print(tokenizer.decode(id))\n",
    "    print(\"----\")\n",
    "decoded = tokenizer.decode(encoded, skip_special_tokens=True)\n",
    "print(len((decoded.replace(\" \", \"\"))))\n",
    "decoded\n",
    "#tokenizer.decode(encoded.ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "934ac7b7-e0e4-478c-b53a-369a021f4982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " '...',\n",
       " '</s>',\n",
       " '<BLANK_SPACE>',\n",
       " '<COLUMN>',\n",
       " '<RULING>',\n",
       " '<SURFACE>',\n",
       " '<mask>',\n",
       " '<pad>',\n",
       " '<s>',\n",
       " '<unk>'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_SPECIAL_TOK_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048d9ae-144a-4650-a7f2-510931e7e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_token_ids(tokenizer, tokens):\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        token_id = tokenizer.token_to_id(token)\n",
    "        if token_id is not None:\n",
    "            token_ids.append((token, token_id))\n",
    "        else:\n",
    "            token_ids.append((token, 'Token not found in vocabulary'))\n",
    "    return token_ids\n",
    "\n",
    "#special_token_ids = get_special_token_ids(tokenizer, SPECIAL_TOKENS)\n",
    "#special_token_ids = set([id for _, id in special_token_ids])\n",
    "#special_token_ids\n",
    "\n",
    "\n",
    "\n",
    "glyph_texts[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
